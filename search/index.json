[{"content":"In my post A Simple Vulkan Compute Example in C++ I described the hello world of Vulkan Compute. A very simple application that squares a vector of integers using a HLSL compute shader. It is quite an involved process, requiring many steps to be completed before getting to the actual compute shader execution. You need to:\n Create a Vulkan Instance, Physical Device, Logical Device Find the flags required to create a compute Queue Create the buffers that the shader will operate on:  Query the memory requirements for a particular buffer Find the index of the memory type to create the buffer from Allocate Memory for the buffers Map the memory and fill it with the data you want Bind the buffers to the memory   Create a Descriptor Set, Shader Module, Pipeline Create a Command Pool, Command Buffer, Fences Dispatch the shader Wait for completion Map the buffers and read the results back  It is a rather complex process just to run some program on your GPU.\nI deliberately split the buffer creation steps into multiple sub-steps to highlight how difficult it can be to manage memory in a Vulkan application. Note that the history repeats for DirectX 12. You need to be aware of the different types of memory and you need to be clever when allocating memory and binding buffers to it. Ideally you allocate a large chunk of memory with several buffers bound to it, when buffers get destroyed you just free the memory in the the pool leaving it available for another allocation. Allocations are quite expensive, so avoiding allocating memory is a good idea.\nFortunately there is a super easy-to-use, open-source, MIT licensed library developed by AMD as part of their GPU-Open initiative that helps you to manage memory in your Vulkan application. The Vulkan Memory Allocator, or VMA for short\nThe Vulkan Memory Allocator (VMA) Library Some useful links:\n Vulkan Memory Allocator on Github Detailed documentation of the Vulkan Memory Allocator library  Basic usage This is a single-header C++ library with a C interface. You can just put the header in your repository and done, installed. You just need to include the following:\n#define VMA_IMPLEMENTATION #include \u0026#34;vk_mem_alloc.h\u0026#34;The VMA_IMPLEMENTATION macro needs to be defined in a single source file or you will get linker errors, so it is recommended that you create a dedicated translation unit just for that.\nYou start by creating an allocator:\nVmaAllocatorCreateInfo AllocatorInfo = {}; AllocatorInfo.vulkanApiVersion = DeviceProps.apiVersion; AllocatorInfo.physicalDevice = PhysicalDevice; AllocatorInfo.device = Device; AllocatorInfo.instance = Instance; VmaAllocator Allocator; vmaCreateAllocator(\u0026amp;AllocatorInfo, \u0026amp;Allocator); Then you can start creating buffers:\nVkBuffer InBufferRaw; VkBuffer OutBufferRaw; VmaAllocationCreateInfo AllocationInfo = {}; AllocationInfo.usage = VMA_MEMORY_USAGE_CPU_TO_GPU; VmaAllocation InBufferAllocation; vmaCreateBuffer(Allocator, \u0026amp;static_cast\u0026lt;VkBufferCreateInfo\u0026gt;(BufferCreateInfo), \u0026amp;AllocationInfo, \u0026amp;InBufferRaw, \u0026amp;InBufferAllocation, nullptr); AllocationInfo.usage = VMA_MEMORY_USAGE_GPU_TO_CPU; VmaAllocation OutBufferAllocation; vmaCreateBuffer(Allocator, \u0026amp;static_cast\u0026lt;VkBufferCreateInfo\u0026gt;(BufferCreateInfo), \u0026amp;AllocationInfo, \u0026amp;OutBufferRaw, \u0026amp;OutBufferAllocation, nullptr); vk::Buffer InBuffer = InBufferRaw; vk::Buffer OutBuffer = OutBufferRaw; int32_t* InBufferPtr = nullptr; vmaMapMemory(Allocator, InBufferAllocation, reinterpret_cast\u0026lt;void**\u0026gt;(\u0026amp;InBufferPtr)); for (int32_t I = 0; I \u0026lt; NumElements; ++I) { InBufferPtr[I] = I; } vmaUnmapMemory(Allocator, InBufferAllocation); Note the reduction of lines of code compared to the raw Vulkan way of allocating memory, the library handles the details for you, and what\u0026rsquo;s best, it does it very efficiently.\nVisualising Memory Allocations One very neat feature of this library is its capability to display your memory allocations in a visual way.\nAt any point in your program you can build a stats string, which is a json file containing the internals of the library. This json can then be fed into a tool that comes with the library to generate an image that shows you how your application is using the memory, you can see more information on the VmaDumpVis on Github, but you can basically do this:\nchar* StatsString = nullptr; vmaBuildStatsString(Allocator, \u0026amp;StatsString, true); { std::ofstream OutStats{ \u0026#34;VmaStats.json\u0026#34; }; OutStats \u0026lt;\u0026lt; StatsString; } vmaFreeStatsString(Allocator, StatsString); And then run the VmaDumpVis.py script with this file to get an image with a snapshot of your memory allocations. Pretty cool!\nTo conclude, this library is super useful, efficient and easy to integrate and you should really use it in your Vulkan application.\nI have updated my Vulkan Compute Sample on Github to include an example of how to integrate VMA in an application. You can disable VMA by simply commenting the line\n// Comment this to disable VMA support #define WITH_VMA This shows how easy it is to replace memory allocations in Vulkan with VMA\n","date":"2021-08-22","permalink":"https://mcleary.github.io/posts/vulkan-memory-allocator/","tags":["vulkan","vulkan-compute","GPGPU","memory"],"title":"Easy Memory Management with the Vulkan Memory Allocator"},{"content":"In Unreal Engine\u0026rsquo;s terminology, a Data Asset is an asset that stores, guess what, data. It can be used, for example, to decouple the configuration from behaviour.\nData Assets in Unreal Engine all inherit from the UDataAsset. You can define a new Data Asset type by inheriting from UDataAsset and then your new data asset will become available in the Pick Data Asset Class Dialog:\nI\u0026rsquo;m used to create new data assets using C++, you inherit from UDataAsset, add some members with UPROPERTY and done, however, recently I wanted to create one using only Blueprints. Looking into existing tutorials on data assets, people usually say that you must create your data asset type in C++ first.\nThat is actually true, but only if you want to inherit directly from UDataAsset. But there is a way to create a new data assets using only blueprints.\nThe procedure is as follows:\n Create a new Blueprint class that inherits from UPrimaryDataAsset Add variables in your Blueprint Your blueprint class will now be available in the Pick Data Asset Class Dialog.  Here is a simple example of a blueprint that inherits from UPrimaryDataAsset with three variables:\nAnd the blueprint type showing up as a new data asset type:\nWhen you create a new asset of this type and double click in it, you can see that all the variables from the blueprint class are now fields of the new data asset:\nSo this is how you create a new data asset type using only blueprints, no C++ involved.\n","date":"2021-07-24","permalink":"https://mcleary.github.io/posts/unreal-data-assets-from-blueprints/","tags":["Unreal Engine","UE4","UE5","Assets","GameDev"],"title":"Creating Unreal Engine Data Assets using only Blueprints"},{"content":"Vulkan is great. It provides a cross-platform API to write applications that use the GPU to do graphics and general purpose compute. Designed from the ground-up to be a modern API, using Vulkan can be quite difficult so you better know what you\u0026rsquo;re doing if you plan to use Vulkan for your application.\nVulkan provides both a graphics and compute APIs and in this post I will focus on the compute part of it as I\u0026rsquo;m still not very familiar with the graphics side of it.\nI had a difficult time searching for simple Vulkan compute samples as the official Khronos Vulkan-Samples only include more elaborate examples. My goal was to write the minimal amount of code to get a compute shader running using Vulkan.\nI found two very useful resources that do more or less what I wanted:\n A Simple Vulkan Compute Example by Neil Henning Vulkan Compute Example by Slava Savenko  Neil\u0026rsquo;s post is great as it goes straight into the code but it uses the C API from Vulkan, so very verbose. I wanted to use vulkan.hpp as it provides a nice C++ interface to use Vulkan. The Vulkan C++ bindings are almost a one-to-one match with the C API, so theoretically I could just port Neil\u0026rsquo;s code to use the Vulkan C++ header but I found this task was not that simple. Slava\u0026rsquo;s sample did just that, however, as with most Vulkan samples out there, there is an infrastructure around the code to make it easier to write, namely classes with a lot of methods to abstract some of the resource management required to use Vulkan.\nDespite that, both links proved to be really useful when writing my own sample.\nThe goal is simple: Run a compute shader that squares the numbers from an input buffer and stores the results in an output buffer, i.e., run the equivalent of the following code but in the GPU, using Vulkan:\nstd::vector\u0026lt;int\u0026gt; Input, Output; for (int I = 0; I \u0026lt; Input.size(); ++I) { Output[I] = Input[I] * Input[I]; } So let\u0026rsquo;s start writing this program.\nInfrastructure  Make sure you have the Vulkan SDK installed on your machine.\n I\u0026rsquo;m assuming you have the Vulkan SDK installed on your machine. For this program I\u0026rsquo;m going to use CMake and HLSL for the shader part.\nThe following CMake file can be used to build the program. I chose to compile the shader to SPIR-V ahead of time to make things simple in the C++ side. Using add_custom_target you can create a CMake target that will compile the Square.hlsl shader when building the project.\ncmake_minimum_required(VERSION 3.16)project(VulkanCompute)find_package(Vulkan REQUIRED)add_custom_command( OUTPUT \u0026#34;${CMAKE_BINARY_DIR}/Square.spv\u0026#34; COMMAND $ENV{VK_SDK_PATH}/Bin/dxc -T cs_6_0 -E \u0026#34;Main\u0026#34; -spirv -fvk-use-dx-layout -fspv-target-env=vulkan1.1 -Fo \u0026#34;${CMAKE_BINARY_DIR}/Square.spv\u0026#34; \u0026#34;Square.hlsl\u0026#34; DEPENDS \u0026#34;Square.hlsl\u0026#34; WORKING_DIRECTORY ${CMAKE_CURRENT_SOURCE_DIR} COMMENT \u0026#34;Buiding Shaders\u0026#34; )add_custom_target(ComputeShader DEPENDS \u0026#34;${CMAKE_BINARY_DIR}/Square.spv\u0026#34;)add_executable(VulkanCompute \u0026#34;main.cpp\u0026#34;)target_link_libraries(VulkanCompute PRIVATE Vulkan::Vulkan)add_dependencies(VulkanCompute ComputeShader)Preamble To use the Vulkan C++ header one just need to\n#include \u0026lt;vulkan/vulkan.hpp\u0026gt;Vulkan Instance - vk::Instance A Vulkan application starts with a vk::Instance, so lets create one:\nvk::ApplicationInfo AppInfo{ \u0026#34;VulkanCompute\u0026#34;, // Application Name  1, // Application Version  nullptr, // Engine Name or nullptr  0, // Engine Version  VK_API_VERSION_1_1 // Vulkan API version }; const std::vector\u0026lt;const char*\u0026gt; Layers = { \u0026#34;VK_LAYER_KHRONOS_validation\u0026#34; }; vk::InstanceCreateInfo InstanceCreateInfo(vk::InstanceCreateFlags(), // Flags  \u0026amp;AppInfo, // Application Info  Layers.size(), // Layers count  Layers.data()); // Layers vk::Instance Instance = vk::createInstance(InstanceCreateInfo); Here I\u0026rsquo;m enabling the VK_LAYER_KHRONOS_validation layer so we can have some help from Vulkan in case something goes wrong.\nEnumerating the Physical Devices - vk::PhysicalDevice A vk::PhysicalDevice represents, as the name suggests, the physical piece of hardware that we can use to run our application. We need to select a physical device from which we can create a logical device, vk::Device that we use to interact with it:\nvk::PhysicalDevice PhysicalDevice = Instance.enumeratePhysicalDevices().front(); vk::PhysicalDeviceProperties DeviceProps = PhysicalDevice.getProperties(); std::cout \u0026lt;\u0026lt; \u0026#34;Device Name : \u0026#34; \u0026lt;\u0026lt; DeviceProps.deviceName \u0026lt;\u0026lt; std::endl; const uint32_t ApiVersion = DeviceProps.apiVersion; std::cout \u0026lt;\u0026lt; \u0026#34;Vulkan Version : \u0026#34; \u0026lt;\u0026lt; VK_VERSION_MAJOR(ApiVersion) \u0026lt;\u0026lt; \u0026#34;.\u0026#34; \u0026lt;\u0026lt; VK_VERSION_MINOR(ApiVersion) \u0026lt;\u0026lt; \u0026#34;.\u0026#34; \u0026lt;\u0026lt; VK_VERSION_PATCH(ApiVersion); vk::PhysicalDeviceLimits DeviceLimits = DeviceProps.limits; std::cout \u0026lt;\u0026lt; \u0026#34;Max Compute Shared Memory Size: \u0026#34; \u0026lt;\u0026lt; DeviceLimits.maxComputeSharedMemorySize / 1024 \u0026lt;\u0026lt; \u0026#34; KB\u0026#34; \u0026lt;\u0026lt; std::endl; Here I\u0026rsquo;m just printing some information from the first physical device available in the machine.\nQueue Family Index - vk::QueueFamilyProperties We need a vk::Queue where we submit work to be done by the device, hopefully a GPU in this case. Note that extra code is required to make sure the selected device is the one you want, in case there are more than one available.\nVulkan supports different types of queues, so we need to query which queue family we need to create a queue suitable for compute work:\nstd::vector\u0026lt;vk::QueueFamilyProperties\u0026gt; QueueFamilyProps = PhysicalDevice.getQueueFamilyProperties(); auto PropIt = std::find_if(QueueFamilyProps.begin(), QueueFamilyProps.end(), [](const vk::QueueFamilyProperties\u0026amp; Prop) { return Prop.queueFlags \u0026amp; vk::QueueFlagBits::eCompute; }); const uint32_t ComputeQueueFamilyIndex = std::distance(QueueFamilyProps.begin(), PropIt); std::cout \u0026lt;\u0026lt; \u0026#34;Compute Queue Family Index: \u0026#34; \u0026lt;\u0026lt; ComputeQueueFamilyIndex \u0026lt;\u0026lt; std::endl; This will select a queue that has compute capabilities. This is equivalent to searching for a queue with the VK_QUEUE_COMPUTE_BIT flag using the C API.\nVulkan Device - vk::Device Creating a device requires a vk::DeviceQueueCreateInfo and a vk::DeviceCreateInfo:\nvk::DeviceQueueCreateInfo DeviceQueueCreateInfo(vk::DeviceQueueCreateFlags(), // Flags  ComputeQueueFamilyIndex, // Queue Family Index  1); // Number of Queues vk::DeviceCreateInfo DeviceCreateInfo(vk::DeviceCreateFlags(), // Flags  DeviceQueueCreateInfo); // Device Queue Create Info struct vk::Device Device = PhysicalDevice.createDevice(DeviceCreateInfo); Allocating Memory Allocating memory in Vulkan is a pain. There libraries that facilitate this task like AMD\u0026rsquo;s Vulkan Memory Allocator, but using this libraries is out of the scope for this post, also I wanted to see how to do it manually first.\nVulkan separates buffers memory. Buffers in Vulkan are just a view into a piece of memory that you also need to manually configure. Allocating memory then is split into three parts:\n Create the required buffers for the application Allocate the memory to back the buffers Bind the buffers to the memory  This separation allows programmers to fine tune memory usage by allocating a large chunk of memory for several buffers for example. For the sake of simplicity, each vk::Buffer will have its corresponding vk::Memory associated with it:\nCreating the buffers - vk::Buffer I\u0026rsquo;m going to create two buffers with 10 elements each:\nconst uint32_t NumElements = 10; const uint32_t BufferSize = NumElements * sizeof(int32_t); vk::BufferCreateInfo BufferCreateInfo{ vk::BufferCreateFlags(), // Flags  BufferSize, // Size  vk::BufferUsageFlagBits::eStorageBuffer, // Usage  vk::SharingMode::eExclusive, // Sharing mode  1, // Number of queue family indices  \u0026amp;ComputeQueueFamilyIndex // List of queue family indices }; vk::Buffer InBuffer = Device.createBuffer(BufferCreateInfo); vk::Buffer OutBuffer = Device.createBuffer(BufferCreateInfo); Allocating memory To allocate memory in Vulkan we first need to find the type of memory we actually require to back the buffers we have. The vk::Device provides a member function called vk::Device::getBufferMemoryRequirements that returns a vk::MemoryRequirements object with information so we can ask Vulkan how much memory to allocate for each buffer:\nvk::MemoryRequirements InBufferMemoryRequirements = Device.getBufferMemoryRequirements(InBuffer); vk::MemoryRequirements OutBufferMemoryRequirements = Device.getBufferMemoryRequirements(OutBuffer); With this information on hand we can query Vulkan for the memory type required to allocate memory that is visible from the host, i.e., memory that can be mapped on the host side:\nvk::PhysicalDeviceMemoryProperties MemoryProperties = PhysicalDevice.getMemoryProperties(); uint32_t MemoryTypeIndex = uint32_t(~0); vk::DeviceSize MemoryHeapSize = uint32_t(~0); for (uint32_t CurrentMemoryTypeIndex = 0; CurrentMemoryTypeIndex \u0026lt; MemoryProperties.memoryTypeCount; ++CurrentMemoryTypeIndex) { vk::MemoryType MemoryType = MemoryProperties.memoryTypes[CurrentMemoryTypeIndex]; if ((vk::MemoryPropertyFlagBits::eHostVisible \u0026amp; MemoryType.propertyFlags) \u0026amp;\u0026amp; (vk::MemoryPropertyFlagBits::eHostCoherent \u0026amp; MemoryType.propertyFlags)) { MemoryHeapSize = MemoryProperties.memoryHeaps[MemoryType.heapIndex].size; MemoryTypeIndex = CurrentMemoryTypeIndex; break; } } std::cout \u0026lt;\u0026lt; \u0026#34;Memory Type Index: \u0026#34; \u0026lt;\u0026lt; MemoryTypeIndex \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34;Memory Heap Size : \u0026#34; \u0026lt;\u0026lt; MemoryHeapSize / 1024 / 1024 / 1024 \u0026lt;\u0026lt; \u0026#34; GB\u0026#34; \u0026lt;\u0026lt; std::endl; And finally we can ask the device to allocate the required memory for our buffers:\nvk::MemoryAllocateInfo InBufferMemoryAllocateInfo(InBufferMemoryRequirements.size, MemoryTypeIndex); vk::MemoryAllocateInfo OutBufferMemoryAllocateInfo(OutBufferMemoryRequirements.size, MemoryTypeIndex); vk::DeviceMemory InBufferMemory = Device.allocateMemory(InBufferMemoryAllocateInfo); vk::DeviceMemory OutBufferMemory = Device.allocateMemory(InBufferMemoryAllocateInfo); The last step of the memory allocation part is to get a mapped pointer to this memory that can be used to copy data from the host to the device. For this simple example I\u0026rsquo;m just setting the value of each element as its index:\nint32_t* InBufferPtr = static_cast\u0026lt;int32_t*\u0026gt;(Device.mapMemory(InBufferMemory, 0, BufferSize)); for (int32_t I = 0; I \u0026lt; NumElements; ++I) { InBufferPtr[I] = I; } Device.unmapMemory(InBufferMemory); Binding Buffers to Memory Finally we can bind the buffers to the allocated memory:\nDevice.bindBufferMemory(InBuffer, InBufferMemory, 0); Device.bindBufferMemory(OutBuffer, OutBufferMemory, 0); And that concludes the memory allocation part of the program.\nCreating the Compute Pipeline - vk::Pipeline The next part of the process is to create the compute pipeline that will be used to run the compute shader on the GPU. Lets start with the HLSL shader we are going to run:\n[[vk::binding(0, 0)]] RWStructuredBuffer\u0026lt;int\u0026gt; InBuffer;\r[[vk::binding(1, 0)]] RWStructuredBuffer\u0026lt;int\u0026gt; OutBuffer;\r[numthreads(1, 1, 1)]\rvoid Main(uint3 DTid : SV_DispatchThreadID)\r{\rOutBuffer[DTid.x] = InBuffer[DTid.x] * InBuffer[DTid.x];\r}\rThe only difference from a regular HLSL shader are the annotations for each buffer. [[vk::binding(1, 0)]] means that a buffer will use binding 1 from descriptor set 0. Descriptor sets can be thought of a way to tell Vulkan how the buffers we defined in the previous section are going to be passed to the pipeline. If you have an existing HLSL shader that you want to use but you can\u0026rsquo;t change the its source code to have the bindings, you can still set the bindings in the command line using dxc using the -fvk-bind-globals argument.\nWith the CMake setup defined in the beginning of the post, there will be a file called Square.spv in the build folder for the project. This file contains the SPIR-V bytecode representing this shader. The goal here is to load this shader as the compute stage of our pipeline.\nShader Module - vk::ShaderModule We start by reading the contents of the SPIR-V file and creating a vk::ShaderModule:\nstd::vector\u0026lt;char\u0026gt; ShaderContents; if (std::ifstream ShaderFile{ \u0026#34;Square.spv\u0026#34;, std::ios::binary | std::ios::ate }) { const size_t FileSize = ShaderFile.tellg(); ShaderFile.seekg(0); ShaderContents.resize(FileSize, \u0026#39;\\0\u0026#39;); ShaderFile.read(ShaderContents.data(), FileSize); } vk::ShaderModuleCreateInfo ShaderModuleCreateInfo( vk::ShaderModuleCreateFlags(), // Flags  ShaderContents.size(), // Code size  reinterpret_cast\u0026lt;const uint32_t*\u0026gt;(ShaderContents.data())); // Code vk::ShaderModule ShaderModule = Device.createShaderModule(ShaderModuleCreateInfo); Descriptor Set Layout - vk::DescriptorSetLayout Next we define the vk::DescriptorSetLayout. This object will tell Vulkan the layout of data to be passed into to the pipeline. Note that this is not the actual descriptor set, it is just the layout of the thing. This got me confused when I first saw it. The actual descriptor set is represented by a vk::DescriptorSet and it needs to be allocated using a descriptor pool. Vulkan is complicated, but you have control!\nLet\u0026rsquo;s define the vk::DescriptorSetLayout:\nconst std::vector\u0026lt;vk::DescriptorSetLayoutBinding\u0026gt; DescriptorSetLayoutBinding = { {0, vk::DescriptorType::eStorageBuffer, 1, vk::ShaderStageFlagBits::eCompute}, {1, vk::DescriptorType::eStorageBuffer, 1, vk::ShaderStageFlagBits::eCompute} }; vk::DescriptorSetLayoutCreateInfo DescriptorSetLayoutCreateInfo( vk::DescriptorSetLayoutCreateFlags(), DescriptorSetLayoutBinding); vk::DescriptorSetLayout DescriptorSetLayout = Device.createDescriptorSetLayout(DescriptorSetLayoutCreateInfo); The vk::DescriptorSetLayout is specified using a series of vk::DescriptorSetLayoutBinding objects. Each binding will assign an index to a buffer in the pipeline. With the vk::DescriptorSetLayout created we can move to create the layout of our compute pipeline. Yes, another layout, not the actual thing!\nPipeline Layout - vk::PipelineLayout vk::PipelineLayoutCreateInfo PipelineLayoutCreateInfo(vk::PipelineLayoutCreateFlags(), DescriptorSetLayout); vk::PipelineLayout PipelineLayout = Device.createPipelineLayout(PipelineLayoutCreateInfo); vk::PipelineCache PipelineCache = Device.createPipelineCache(vk::PipelineCacheCreateInfo()); Pipeline - vk::Pipeline Now we can finally create the compute pipeline:\nvk::PipelineShaderStageCreateInfo PipelineShaderCreateInfo( vk::PipelineShaderStageCreateFlags(), // Flags  vk::ShaderStageFlagBits::eCompute, // Stage  ShaderModule, // Shader Module  \u0026#34;Main\u0026#34;); // Shader Entry Point vk::ComputePipelineCreateInfo ComputePipelineCreateInfo( vk::PipelineCreateFlags(), // Flags  PipelineShaderCreateInfo, // Shader Create Info struct  PipelineLayout); // Pipeline Layout vk::Pipeline ComputePipeline = Device.createComputePipeline(PipelineCache, ComputePipelineCreateInfo); Creating the vk::DescriptorSet Descriptor sets must be allocated in a vk::DescriptorPool, so we need to create one first:\nvk::DescriptorPoolSize DescriptorPoolSize(vk::DescriptorType::eStorageBuffer, 2); vk::DescriptorPoolCreateInfo DescriptorPoolCreateInfo(vk::DescriptorPoolCreateFlags(), 1, DescriptorPoolSize); vk::DescriptorPool DescriptorPool = Device.createDescriptorPool(DescriptorPoolCreateInfo); Now we can finally allocate the descriptor set and update them to use our buffers:\nvk::DescriptorSetAllocateInfo DescriptorSetAllocInfo(DescriptorPool, 1, \u0026amp;DescriptorSetLayout); const std::vector\u0026lt;vk::DescriptorSet\u0026gt; DescriptorSets = Device.allocateDescriptorSets(DescriptorSetAllocInfo); vk::DescriptorSet DescriptorSet = DescriptorSets.front(); vk::DescriptorBufferInfo InBufferInfo(InBuffer, 0, NumElements * sizeof(int32_t)); vk::DescriptorBufferInfo OutBufferInfo(OutBuffer, 0, NumElements * sizeof(int32_t)); const std::vector\u0026lt;vk::WriteDescriptorSet\u0026gt; WriteDescriptorSets = { {DescriptorSet, 0, 0, 1, vk::DescriptorType::eStorageBuffer, nullptr, \u0026amp;InBufferInfo}, {DescriptorSet, 1, 0, 1, vk::DescriptorType::eStorageBuffer, nullptr, \u0026amp;OutBufferInfo}, }; Device.updateDescriptorSets(WriteDescriptorSets, {}); Submitting the work to the GPU Command Pool - vk::CommandPool To actually run this shader on the GPU we need to submit the work on a vk::Queue. We tell the queue to commands stored in a one or more vk::CommandBuffers. Commands must be allocated in a vk::CommandPool, so we need to create one first:\nvk::CommandPoolCreateInfo CommandPoolCreateInfo(vk::CommandPoolCreateFlags(), ComputeQueueFamilyIndex); vk::CommandPool CommandPool = Device.createCommandPool(CommandPoolCreateInfo); Command Buffers - vk::CommandBuffer Now we can use the command pool to allocate one or more command buffers:\nvk::CommandBufferAllocateInfo CommandBufferAllocInfo( CommandPool, // Command Pool  vk::CommandBufferLevel::ePrimary, // Level  1); // Num Command Buffers const std::vector\u0026lt;vk::CommandBuffer\u0026gt; CmdBuffers = Device.allocateCommandBuffers(CommandBufferAllocInfo); vk::CommandBuffer CmdBuffer = CmdBuffers.front(); Recording Commands We can now record commands in the vk::CommandBuffer object. To run the compute shader we need to bind the pipeline, descriptor sets and record a vk::CommandBuffer::dispatch call:\nvk::CommandBufferBeginInfo CmdBufferBeginInfo(vk::CommandBufferUsageFlagBits::eOneTimeSubmit); CmdBuffer.begin(CmdBufferBeginInfo); CmdBuffer.bindPipeline(vk::PipelineBindPoint::eCompute, ComputePipeline); CmdBuffer.bindDescriptorSets(vk::PipelineBindPoint::eCompute, // Bind point  PipelineLayout, // Pipeline Layout  0, // First descriptor set  { DescriptorSet }, // List of descriptor sets  {}); // Dynamic offsets CmdBuffer.dispatch(NumElements, 1, 1); CmdBuffer.end(); The vk::CommandBuffer::dispatch function takes the number of threads to launch in the device. In this example we are launching one thread for element.\nWith the vk::CommmandBuffer recorded we can finaly submit the work the GPU. We first get the vk::Queue from the vk::Device using the queue family index retrieved earlier and we create a vk::Fence. The fence is a mechanism we can use to wait for the compute shader to complete. After waiting we can read the results of our computation:\nvk::Queue Queue = Device.getQueue(ComputeQueueFamilyIndex, 0); vk::Fence Fence = Device.createFence(vk::FenceCreateInfo()); vk::SubmitInfo SubmitInfo(0, // Num Wait Semaphores  nullptr, // Wait Semaphores  nullptr, // Pipeline Stage Flags  1, // Num Command Buffers  \u0026amp;CmdBuffer); // List of command buffers Queue.submit({ SubmitInfo }, Fence); Device.waitForFences({ Fence }, // List of fences  true, // Wait All  uint64_t(-1)); // Timeout The final step is to map the output buffer and read the results. For this example we are just going to print the values in the terminal:\nInBufferPtr = static_cast\u0026lt;int32_t*\u0026gt;(Device.mapMemory(InBufferMemory, 0, BufferSize)); for (uint32_t I = 0; I \u0026lt; NumElements; ++I) { std::cout \u0026lt;\u0026lt; InBufferPtr[I] \u0026lt;\u0026lt; \u0026#34; \u0026#34;; } std::cout \u0026lt;\u0026lt; std::endl; Device.unmapMemory(InBufferMemory); int32_t* OutBufferPtr = static_cast\u0026lt;int32_t*\u0026gt;(Device.mapMemory(OutBufferMemory, 0, BufferSize)); for (uint32_t I = 0; I \u0026lt; NumElements; ++I) { std::cout \u0026lt;\u0026lt; OutBufferPtr[I] \u0026lt;\u0026lt; \u0026#34; \u0026#34;; } std::cout \u0026lt;\u0026lt; std::endl; Device.unmapMemory(OutBufferMemory); Cleaning We need to manually delete the resources used by our program or the validation layer will shout at you:\nDevice.resetCommandPool(CommandPool, vk::CommandPoolResetFlags()); Device.destroyFence(Fence); Device.destroyDescriptorSetLayout(DescriptorSetLayout); Device.destroyPipelineLayout(PipelineLayout); Device.destroyPipelineCache(PipelineCache); Device.destroyShaderModule(ShaderModule); Device.destroyPipeline(ComputePipeline); Device.destroyDescriptorPool(DescriptorPool); Device.destroyCommandPool(CommandPool); Device.freeMemory(InBufferMemory); Device.freeMemory(OutBufferMemory); Device.destroyBuffer(InBuffer); Device.destroyBuffer(OutBuffer); Device.destroy(); Instance.destroy(); The Vulkan C++ Header also has a raii set of objects that can be used to avoid manually cleaning resources, for example:\nvk::raii::Context context; vk::raii::Instance instance = vk::raii::su::makeInstance( context, AppName, EngineName ); // enumerate the physicalDevices vk::raii::PhysicalDevices physicalDevices( instance ); So this objects will clean their resources upon destruction.\nConclusion If you are here, well done, you can now use Vulkan to square a few numbers on the GPU =). Hopefully this will be useful to you in staring with Vulkan to run compute workloads on your GPU.\nThis code is pretty much in the order required to build your application so you should be able to copy-and-paste this code into your main function and have some numbers printed in the terminal. You can also find this code in this Github Repo ready to be built and executed.\nSee you next time!\n","date":"2021-07-07","permalink":"https://mcleary.github.io/posts/vulkan-compute-example/","tags":["vulkan","vulkan-compute","GPGPU"],"title":"A Simple Vulkan Compute Example in C++"}]