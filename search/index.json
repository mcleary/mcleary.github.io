[{"content":"A collection of MetaHumans created using the new Mesh to MetaHuman tool in the new MetaHuman Plugin for Unreal Engine 5\nOfficial Documentation Mesh to MetaHuman Mesh to MetaHuman Quick Start Press Coverage fxGuide: Huge update for MetaHuman : Import your own head! AWN: Unreal Engine Mesh to MetaHuman Plugin Now Available 80.tv: Epic Games\u0026rsquo; New Release Brings a Mesh to MetaHuman Feature to UE GameFromScratch: Make Your Own MetaHuman in Unreal Engine 5 CGPress: Epic updated Metahuman with ability to import custom character meshes YouTube Twitter These take a while to load\nTurning Buddha into a #MetaHuman using the new MetaHuman plugin for Unreal Engine 5 #UnrealEngine5 #MeshToMetaHuman pic.twitter.com/giMyvypmUR\n\u0026mdash; Thales (@tluisrs) June 11, 2022 è‡ªåˆ†ã‚’é›‘ã«MetaHumanåŒ–ã—ã¦ã¿ãŸã€€ã–ã£ãã‚Šã¾ã¨ã‚\n1.HeadShotã¨ã„ã†ã‚½ãƒ•ãƒˆã‚’ä½¿ã£ã¦ä¸€æšçµµã‹ã‚‰3Dãƒ¢ãƒ‡ãƒ«ã‚’ç”Ÿæˆ\n2. #UE5 ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆå¾Œã€Mesh to MetaHumanã‚’ä½¿ã£ã¦â†‘ã®ãƒ¢ãƒ‡ãƒ«ã‚’ #MetaHuman ã«\n3. é«ªã‚„ãƒ†ã‚¯ã‚¹ãƒãƒ£ã¯æŒã£ã¦ã“ã‚Œãªã„ã®ã§ã€MetaHumanã€€Creatorä¸Šã§è‰²ã€…èª¿æ•´\n4. ï½³ï½ªï½°ï½² https://t.co/BEX1OnuvbA pic.twitter.com/8IQ9Xk3aCv\n\u0026mdash; ãŠã‹ãš (@pafuhana1213) June 9, 2022 Trying facial scanning before testing the new #MetaHuman plugin. @RealityCapture_ vs @Polycam3D:\nRC looks much better, but keeps saying a bunch of my images can\u0026#39;t align, so I get partial scans. Polycam also has issues importing into Unreal as GLB. App look vs in UE5 on the right pic.twitter.com/nkxlPFjnPv\n\u0026mdash; Alex Coulombe (@iBrews) June 9, 2022 Just tried the custom character mesh feature for @UnrealEngine\u0026#39;s #metahuman on my @streetboyja character... I\u0026#39;m still in shock! Absolutely amazing! #unrealengine5 #UE5 #unrealengine #gamedev #indiedev #3dart #metahuman #characterdesign pic.twitter.com/uBVG5sbPyP\n\u0026mdash; Akeem Pennicooke (@AkeemPennicooke) June 9, 2022 So here\u0026#39;s the new Mesh to Metahuman tool in action. It\u0026#39;s great at getting the overall shape, but the major caveat is that it does *not* transfer any textures, so you\u0026#39;ll need to do some manual texture painting to transfer that. #metahuman #UnrealEngine pic.twitter.com/LjSimNjXhb\n\u0026mdash; Brielle Garcia (47%) (@tacolamp) June 9, 2022 Tried the new #MetaHuman workflow today, took a bulky 3D scan through the process and it was super quick+easy. Just wow, what a game changer. #UnrealEngine5 (process screenshots below) pic.twitter.com/KdieI6Fx0b\n\u0026mdash; ğ–›ğ–†ğ–‘ ğŸ‡ºğŸ‡¦ (@VPestilenZ) June 9, 2022 I threw Cal Kestis from Fallen Order into the new Mesh to Metahuman tools and it seems to work incredibly well. This is huge for short film creators, especially in the Star Wars community. Animating digital characters just got a whole lot easier! pic.twitter.com/9rqhR9HNy8\n\u0026mdash; Frostbite Cinematics î¨€ (@FrostbiteCaps) June 9, 2022 Shit just got real ğŸ‘€ Mesh to Metahuman ğŸ‘ğŸ‘ğŸ‘ pic.twitter.com/ehLDmWCE5P\n\u0026mdash; Abhijit Vinayak (@AbhijitVinayak) June 9, 2022 Created using Mesh to MetaHuman (metahuman plugin). #UE5 #UnrealEngine #GameDev pic.twitter.com/SotWfBYR0Q\n\u0026mdash; ğŸ‡§ğŸ‡· ğŸ”º ĞLEX ğŸ® (@Alex_GameDev_) June 9, 2022 Here\u0026#39;s a more direct comparison of the original scan vs the Metahuman mesh that\u0026#39;s generated. Upon closer inspection, it is *much* more accurate than I thought at first. #metahuman #UnrealEngine pic.twitter.com/UYHEO4fWlB\n\u0026mdash; Brielle Garcia (47%) (@tacolamp) June 9, 2022 MESH TO METAHUMAN - Nueva ActualizaciÃ³n de Unreal Engine\nOs contamos todo lo nuevo en el artÃ­culo!â¬‡ï¸https://t.co/PbQBvohrCW#unrealengine #gamedev #indiedev pic.twitter.com/fJdr5xuMvE\n\u0026mdash; iVisual (@iVisualEstudios) June 10, 2022 ã‚‚ã†ã¡ã‚‡ã£èª¿æ•´ã—ã¦ã‚ªãƒªã‚­ãƒ£ãƒ©ã‚’Mesh to Metahuman ã§å‡ºåŠ›ã—ãŸã‚‰ã€ãªãœã‹æœ‰å‰æ°ã«ä¼¼ã¦ã—ã¾ã£ãŸ!!ğŸ˜†#UE5 #MetaHuman pic.twitter.com/JMopaC3jAF\n\u0026mdash; wajyuu (@wajyuu2) June 10, 2022 Trying out the new Mesh to metahuman plugin \u0026amp; And I was REALLY IMPRESSED ! with the results and How close it got to the Source mesh.\nI Wonder how many people Can guess the celebrity ğŸ˜‰ #EpicGames #metahumans #metahumancreator #UnrealEngine5 #UE5 pic.twitter.com/xAYMxsY3ef\n\u0026mdash; Base Reality (@Base_Reality_VR) June 10, 2022 Well, I turned Agent Flint from @lastroundgame into a Metahuman. It\u0026#39;s... a result pic.twitter.com/4coC34R28X\n\u0026mdash; Joe Wintergreen (@joewintergreen) June 10, 2022 Younger characters also work really well with #MetahumanCreator - using the new Mesh to Metahuman tool from the previous sculpt! #Metahuman #UE5 @UnrealEngine pic.twitter.com/Wu04pMjObV\n\u0026mdash; Charles Logan (@cloganart) June 10, 2022 MESH TO METAHUMAN! COOL AF!\n\u0026mdash; Jon Noorlander (@JonNoorlander) June 9, 2022 More #MetahumanCreator Mesh to Metahuman trials - this time with a scan from #3DScanStore @UnrealEngine pic.twitter.com/8ZIAP5vA2z\n\u0026mdash; Charles Logan (@cloganart) June 9, 2022 1-2-3-done. Have fun with the creator. #MetaHuman #UnrealEngine5 #Metaverse pic.twitter.com/h1yxmeJsFD\n\u0026mdash; æˆ‘çš„æ˜Ÿç³»æ˜¯é‡‘ç‰›åº§ (@iamfromtaurus) June 10, 2022 ã‚¦ã‚½ã ã‚ã€œã€æ–°ã—ã„ #UE5 ã® #Metahuman ã§ä¿ºã®ã‚ªãƒªã‚­ãƒ£ãƒ©ãŒç”Ÿã¾ã‚Œå¤‰ã‚ã£ãŸãƒ¼!!ğŸ¤£ pic.twitter.com/RjNnLvAY9p\n\u0026mdash; wajyuu (@wajyuu2) June 10, 2022 ãƒ•ãƒªãƒ¼ã®é ­éƒ¨ãƒ¢ãƒ‡ãƒ«ã‚’é›‘ã«å–ã‚Šè¾¼ã‚“ã§ã‚‚ã¡ã‚ƒã‚“ã¨ãƒ•ã‚§ã‚¤ã‚¹ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã¾ã§æ©Ÿèƒ½ã—ã¦ã‚‹ã®ã¯å„ªç§€ã ãªã‚ã€‚ã€‚ã€‚\nVtuberã®ãƒœãƒ‡ã‚£ã«ã¾ã§å¿œç”¨ãã‹ã›ã‚‰ã‚ŒãŸã‚‰ã„ã‚ˆã„ã‚ˆæ¼”å‡ºã‚„ãƒ¬ãƒ™ãƒ«ãƒ‡ã‚¶ã‚¤ãƒ³ã®é ˜åŸŸã«æ‰‹ãŒå±Šããã†ã€‚#UE5#MetaHuman pic.twitter.com/02Ef2M1OLw\n\u0026mdash; ã„ã‚ã—ã‚ƒãğŸ¥UEå‹‰å¼·ä¸­ (@iwashaki_write) June 10, 2022 æ—©é€ŸMetaHumanã®ã‚«ã‚¹ã‚¿ãƒ ãƒ¡ãƒƒã‚·ãƒ¥ã§ã²ã‚ã‚†ãã‚’ä½œã‚‹ã‚‚ã€å…¨ç„¶ä¼¼ã¦ã‚‚ä¼¼ã¤ã‹ãªã„é¡”ã«ãªã‚‹ã€‚ #MetaHuman pic.twitter.com/0sN9kfaZLm\n\u0026mdash; ã‚µãƒ¡ã‚¸éƒ¨é•· (@samezi) June 10, 2022 From Scan to #MetaHuman in less than an hour thanks to the new workflow from @epicgames and @unrealengine !#electriclensco #unrealengine #realitycapture pic.twitter.com/SSAhOh6VGK\n\u0026mdash; Matthew Hermans (@renbry) June 10, 2022 Yeah, I also tried Catherine but I need a better hairstyle.#UE5 #Myst #Riven #MetaHuman https://t.co/okscd7i8lS pic.twitter.com/C40Mm7yPAg\n\u0026mdash; Distant Dimensions (@dist_dimensions) June 10, 2022 Mesh to Metahuman experiment. Custom zbrush model to metahuman, this stuff is magic.#metahumancreator #MetaHuman #UnrealEngine5 #madewithunreal pic.twitter.com/P7HCg99A5J\n\u0026mdash; Joshua James (@coldpolygon) June 10, 2022 The latest UE5 Metahuman Creator update is insane.\nI can\u0026#39;t wait to use this tech in future projects! pic.twitter.com/R3uOpYpMlv\n\u0026mdash; Cinematic Captures (@CineCaptures) June 10, 2022 Mesh to Metahuman ÑƒĞ³Ğ°Ñ€ - Ğ½Ğ°ÑˆĞµĞ» Ğ±ĞµÑĞ¿Ğ»Ğ°Ñ‚Ğ½ÑƒÑ 3D Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ»Ğ¸Ñ†Ğ° Ğ“Ğ¾ÑĞ»Ğ¸Ğ½Ğ³Ğ°, Ğ·Ğ° Ğ¿Ğ°Ñ€Ñƒ ĞºĞ»Ğ¸ĞºĞ¾Ğ² Ğ±ĞµĞ· Ğ¾ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¾Ñ‚ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ» Ğ¸Ğ· ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ°Ñ…ÑŒÑĞ¼Ğ°Ğ½Ğ° ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ³Ğ¾Ñ‚Ğ¾Ğ² Ğº Ğ°Ğ½Ğ¸Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ! Ğ”Ğ°, Ğ½Ğµ Ğ¾Ñ‡ĞµĞ½ÑŒ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğµ, Ğ½Ğ¾ ĞµÑĞ»Ğ¸ Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ñ‡Ğ¸Ñ‚ÑŒÑÑ, Ñ‚Ğ¾ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ¹Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚. Ğ­Ñ‚Ğ¾, ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ¾, Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ²! #UE5 #MetaHuman pic.twitter.com/ulGpHjgXuu\n\u0026mdash; Nikolas Winding Samborsky (@NikolsaNNN) June 10, 2022 iphone 11 + scandyPro + mesh to metahuman = your face in #UE5 in no time !#indiedev #gamedevelopment pic.twitter.com/vgTQQWZWbj\n\u0026mdash; Madao (@lulumadao) June 10, 2022 New custom Mesh to Metahuman example in Unreal. Song: \u0026quot;Bezos IV\u0026quot; by Bo Burnham pic.twitter.com/9lJy1E6c66\n\u0026mdash; Brielle Garcia (47%) (@tacolamp) June 10, 2022 Trying out Mesh to MetaHuman with my own scan: here\u0026#39;s the result!\nWhat do you think?\nI would probably add a texturing step after the Mesh to MetaHuman process, and voilÃ !#metahumans #ue5 #unrealengine #unrealengine5 #digitalhumans #3dscanning #characterdesign pic.twitter.com/fv5ZT2s4lv\n\u0026mdash; Alvaro / Kukaracha (@ALToloza) June 10, 2022 @UnrealEngine scanning from a Mesh is truly amazing ğŸ¤©ğŸ”¥ğŸ¤#MetaHuman #metahumancreator #UE5 #UnrealEngine5 pic.twitter.com/66AuwZoz80\n\u0026mdash; yassinerahal (@yassinerahal) June 10, 2022 ì´ë²ˆì— ë°œí‘œëœ UE5ì˜ Mesh to Metahuman ê¸°ëŠ¥ì„ ì´ìš©í•´ì„œ ê¸°ì¡´ì— ë‚´ê°€ ë§Œë“  ëª¨ë¸ë§ì„ Metahumanìœ¼ë¡œ ë§Œë“¤ì–´ë´¤ëŠ”ë° ì•„ì£¼ ì˜ ì‘ë™í•œë‹¤.\nì¢€ë” í›„ì²˜ë¦¬ íŒŒë¼ë©”í„°ê°€ ìˆìœ¼ë©´ ì¢‹ê² ë‹¤ëŠ” ìƒê°ì´ ë“¤ì§€ë§Œ ì•„ì§ í˜„ì¬ ì§„í–‰í˜•ìœ¼ë¡œ ë°œì „ ì¤‘ì¸ íˆ´ì´ë‹ˆê¹Œ ì „í˜€ ë¬¸ì œë˜ì§€ ì•Šê³ .#UE5 #MetaHuman pic.twitter.com/Sz5lI5WOdv\n\u0026mdash; í•œëŒ€í›ˆ / Daehoon Han (@Hanguny) June 10, 2022 MetaHumanPluginã§è‡ªåˆ†ã®3Dãƒ¢ãƒ‡ãƒ«ã‚’ã¨ã‚Šã“ã‚“ã§ã¿ãŸï¼ï¼ è©¦ã™ã®ã¯ç°¡å˜ã€ãã‚Œã£ã½ã„ã®ãŒã§ãã‚ãŒã‚‹ï¼ï¼https://t.co/GaZpgv56IT @YouTubeã‚ˆã‚Š #MetaHuman #UE5\n\u0026mdash; ovjang (@followapp) June 10, 2022 MetaHuman å‹•ç”»ã‚­ãƒ£ãƒ—ãƒãƒ£3\nç›®ã«ãƒ¡ã‚¤ã‚¯ã„ã‚ŒãŸ pic.twitter.com/kLymOxoCEj\n\u0026mdash; h64g (@h64g_) June 10, 2022 Testing #metahumancreator Mesh to #MetaHuman with a stylized sculpt of mine #UE5 #UE5Study pic.twitter.com/WhD8nQEftD\n\u0026mdash; Louis (@AntieDragon) June 10, 2022 Tested MetaHuman\u0026#39;s new plugin with a low poly scan of me and it does wonders. Wish there was an easy way to project/apply scanned textures. Current method is using ZWrap, but if any better methods, then suggestions are appreciated! #MetaHuman #UE5 @UnrealEngine pic.twitter.com/fF7YwbqfJ7\n\u0026mdash; Parth Shah (@parthshah91293) June 10, 2022 é›‘ã«ã‚¹ã‚­ãƒ£ãƒ³ã—ãŸã®ã§Metahuman Identityã§ã¯ã€Men In Blackã«å‡ºã¦ããã†ãªå®‡å®™äººã§ã—ãŸã€‚\nã§ã‚‚ã€ã‚«ã‚¹ã‚¿ãƒ ãƒ¡ãƒƒã‚·ãƒ¥ã§èª¿æ•´ã§ããŸã®ã§ã€äººé¡ã«æˆ»ã‚Œã¾ã—ãŸã€‚(è¬ç½ªï¼šã¡ã‚‡ã£ã´ã‚Šã‚¹ãƒªãƒ ã«ã—ã¾ã—ãŸ) pic.twitter.com/fZ2XMDc3r2\n\u0026mdash; TARKğŸ¥· (@LideTark) June 10, 2022 Refiz minha filhaâ€¦em 3DğŸ˜. #metahuman #unreal pic.twitter.com/cOxg3HTpsU\n\u0026mdash; D5 (@alexandredemaio) June 18, 2022 ğŸ˜œ\n\u0026quot;Hice\u0026quot; a Dua Lipa con Meta Human de Unreal Engine 5.\nPartÃ­ desde blender, luego el nuevo plugin de MetaHuman para UE5.#Unrealengine5 pic.twitter.com/Dyp7TWgz2C\n\u0026mdash; ğŸ‘â€ğŸ—¨ Incoherencias coherentes ğŸ¤– (@tweetsperdidos) June 18, 2022 Seems legit #metahuman pic.twitter.com/FnS51gNnzv\n\u0026mdash; Zarkua (@artzarkua) June 17, 2022 mindblowing! Sculpture A and #Metahuman B.\nFrederick VI of Denmark pic.twitter.com/Vjmsph3D3w\n\u0026mdash; Zarkua (@artzarkua) June 17, 2022 ğŸ‘€ Whoâ€™s tested the new Mesh to MetaHuman feature yet?\nOur Creative Director Sam has released a great workflow for the new #metahuman plugin ğŸ’«\nWatch it here: https://t.co/O3qXq7L6fN#madewithrokoko #unrealengine #ue5 pic.twitter.com/5hgXuUKVwR\n\u0026mdash; Rokoko (@hellorokoko) June 17, 2022 I created some #metahuman characters based on my #DazStudio characters Alita and Eren! It\u0026#39;s so fun! @EpicGames @UnrealEngine pic.twitter.com/DNAwJ9ByID\n\u0026mdash; MoonscapeGraphics (@retro_devil) June 16, 2022 To do this import a model with the Metahuman plugin enabled (it\u0026#39;s free) and create an identity asset. Select Component From Mesh and select your mesh. Frame it up nicely and right click the bottom and click \u0026quot;Track Markers\u0026quot;. Clean them up as required. pic.twitter.com/hs1fGI9o9j\n\u0026mdash; Chris Murphy (@HighlySpammable) June 16, 2022 I made emperor #Trajan as a living human being, using @unrealengine Metahuman creator. Using @UffiziGalleries bust of Trajan. #rome #emperor #romanarchaeology #Metahuman #metahumancreator #photoscan #UE5 #History #antiquity #romanhistory pic.twitter.com/i3UpCIghp4\n\u0026mdash; Elias Artista ğŸ‡ºğŸ‡¦ ğŸ‡¨ğŸ‡¦ (@artista_elias) June 15, 2022 After yesterdayâ€™s experiment with #UnrealEngineâ€™s #MetaHuman, I wanted to try #UE5 #LiveLink in order to animate my #MetaManu #DigitalTwin in #RealTime3D using my phoneâ€™s camera. I also included some high quality animations from #MetaHumanCreator. cc @UnrealEngine pic.twitter.com/lP0p6mNfFm\n\u0026mdash; Manu.Vision (@ManuVision) June 15, 2022 I followed in the footsteps of many others and tested Mesh to MetaHuman in #UnrealEngine5.\nA quick ğŸ§µ on the process: pic.twitter.com/WqEeWM59HP\n\u0026mdash; metamike (@itsmetamike) June 15, 2022 ç©ä¸€ä¸‹MetaHuman pic.twitter.com/5fweX7V3xi\n\u0026mdash; MaxNuke (@Max_Nuke) June 15, 2022 äººã®é¡”ã®3Dãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹ã§ã‚¹ãƒ¼ãƒ‘ãƒ¼ãƒ•ã‚©ãƒˆãƒªã‚¢ãƒ«ãªã‚¢ãƒã‚¿ãƒ¼ä½œã‚Œã‚‹Metahumanã®æ–°ã—ã„æ©Ÿèƒ½ã‚’ä½¿ã£ã¦ã€ã‚¨ã‚¸ãƒ—ãƒˆåšç‰©é¤¨ã«ã‚ã‚‹ãƒãƒ•ã‚§ãƒ«ãƒ†ã‚£ãƒ†ã‚£èƒ¸åƒã®3Dãƒ¢ãƒ‡ãƒ«ã‹ã‚‰Metahumanä½œã£ã¦ã‚‹ã®ã¨ã¦ã‚‚è‰¯ã„ã€‚ãƒ•ã‚¡ãƒ©ã‚ªã®å«ãŒç”Ÿãè¿”ã£ãŸ\npic.twitter.com/nw2xjDzwd2\n\u0026mdash; Falcon â—‡ ãƒ¡ã‚¿ãƒãƒ¼ã‚¹ãƒ»XR (@makotofalcon) June 15, 2022 Nefertiti as a #Metahuman. I was curious to test the tools; wild stuff. (3D Scan cleanup in #blender, followed by #UE5 \u0026amp; #metahumancreator .) Original Berlin Neues Museum source scans available online : https://t.co/ZFwsRKxdgP pic.twitter.com/9ZOFlqXH3B\n\u0026mdash; Jason Bennett (@B0B0S0N) June 14, 2022 è©¦ã•ã–ã‚‹ã‚’å¾—ãªã‹ã£ãŸScan to MetaHuman pic.twitter.com/dHKSLjSPgO\n\u0026mdash; ã‚¹ã‚­ãƒ£ãƒ³ã‚½ãƒ³ / CHP (@scanson3) June 14, 2022 Clonando a Ellie en #UnrealEngine #metahuman #efxtivostudio pic.twitter.com/mzrFofnji4\n\u0026mdash; efxtivo (@efxtivo) June 13, 2022 Tommy Vercetti Unreal Engine 5 x MetaHuman Mesh to Metahuman system is kinda scary pic.twitter.com/SjwKP7AiNE\n\u0026mdash; WeeGee. (@WizzyWage) June 13, 2022 Niko Bellic Unreal Engine x MetaHuman first attempt at mesh import not bad pic.twitter.com/zBLl55mQ1k\n\u0026mdash; WeeGee. (@WizzyWage) June 13, 2022 I have rendered Ceasar unto Metahuman. Using a scan created by the The British Museum and doing some adjustments in zbrush. #metahumans #sketchfab #Epic #UnrealEngine #zbrush pic.twitter.com/2ux5kdKQzn\n\u0026mdash; Vertices UVs normals polygon stuff. (@JustinMapes) June 12, 2022 I have tested Mesh to Metahuman Plugin again starting from a mesh created with photogrammetry. I think now it\u0026#39;s starting to look more like my head. Now it\u0026#39;s time to work on the details ğŸ˜ #UE5 #UnrealEngine5 #CGI #metahumans pic.twitter.com/TpzIxljR9w\n\u0026mdash; Ricardo GonzÃ¡lez (@ricardogonsan) June 12, 2022 UE5 Mesh to Metahuman Testing ...https://t.co/psYuPn0jF9#metahuman #epic #UnrealEngine #UE5 pic.twitter.com/RnfuGpmTh9\n\u0026mdash; Bao Le (@_xmasjazzy_) June 12, 2022 MetaHuman å°‚ç”¨ å…ƒãƒ¡ãƒƒã‚·ãƒ¥\nå”‡ã¯ä¸Šä¸‹ã§åˆ¥è‰²ã€‚ã»ã†ã‚Œã„ç·šã‚‚è‰²åˆ†ã‘\nå˜è‰²ãªã®ã§ã€UE5ã§ã€Œãƒ©ã‚¤ãƒ†ã‚£ãƒ³ã‚°ã‚ã‚Šã€ã§ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã™ã‚‹ã¨ãƒ©ã‚¤ãƒ³ä¿®æ­£ã»ã¼ãƒŠã‚·ã§ã„ã‘ãŸã€‚ pic.twitter.com/cEZfJPLKoD\n\u0026mdash; h64g (@h64g_) June 12, 2022 Scan to MetaHuman Tutorial for Unreal EngineÂ 5 https://t.co/VgJAwrqgNf\n\u0026mdash; Ka2studio (@Ka2_Studio) June 12, 2022 #scarlettjohansson with another haircut in #MetaHuman #UE5 @UnrealEngine pic.twitter.com/tyuQ67vLn2\n\u0026mdash; AFredenucci (@afredenucci_) June 12, 2022 my old low poly character in #metahuman #ue5 pic.twitter.com/rsHJqDK1RM\n\u0026mdash; 3d rat (@3dRatte) June 11, 2022 me, converted to an Unreal Engine Metahuman. It\u0026#39;ll be better if I make a custom head hair groom, and glasses. #UnrealEngine5 pic.twitter.com/v7EtbYVxi3\n\u0026mdash; Bert Huntsinger (@amethyst_videos) June 11, 2022 Mesh to #MetaHuman, J\u0026#39;ai effectue mon petit test\nJe trouve ca sympa. Dans mon cas je ne suis pas parti d\u0026#39;un scan, mais d\u0026#39;un sculpt zbrush d\u0026#39;une statue. Ca retient certain trait du visage mais vraiment pas tout. Globalement je trouve ca plutot bien.\nVous savez qui est ce ? pic.twitter.com/LNzo6Vv2FJ\n\u0026mdash; Fred Unreal Zone #UE5 (@fred_unreal) June 11, 2022 OMG ğŸ˜± mesh to #MetaHuman supports imported head scans for #ue5 @UnrealEngine @quixeltools pic.twitter.com/9dRwzLeywU\n\u0026mdash; Don Allen Stevenson III (@DonAllenIII) June 11, 2022 Mesh to metahumanã§å¤ã„ãƒ¢ãƒ‡ãƒ«ãŒç„¡äº‹ã«Metahumanã«å¤‰æ›ã•ã‚ŒãŸã€‚ã‚ã‚‹ç¨‹åº¦ãƒ‡ãƒ•ã‚©ãƒ«ãƒ¡ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚‚æ„å¤–ã¨ã„ã‘ã‚‹ãŒã€ç›®ã®å¤§ãã•ã‚„é¡”ã®ã‚·ãƒ«ã‚¨ãƒƒãƒˆãªã©é™ç•ŒãŒã‚ã‚‹ã‚‰ã—ã„ã€‚#MetaHuman #UE5 pic.twitter.com/GLvSVa0Zoi\n\u0026mdash; LEV (@lev978) June 11, 2022 Metahooman#UE5 #MetaHuman #3dart pic.twitter.com/ZsqCQ4GhWn\n\u0026mdash; Emmanuel Galdones (@BlobEmman) June 11, 2022 So, the new mesh to #MetaHuman plugin was able to transfer my character\u0026#39;s ears and the shape of the eyebrows flawlessly. Here you can see the original mesh and the trackers:#3dart #UnrealEngine5 #ue5 pic.twitter.com/5qA97XmuCh\n\u0026mdash; AndrÃ© M. (@AndreMailho) June 11, 2022 Queen Nefertiti based from the sculpture of the Egyptian Museum of Berlin. #MetaHuman #Metaverse #UnrealEngine #3dart #gamedev pic.twitter.com/GUyfcptrjq\n\u0026mdash; RunJumpFilms (@runjumpfilms) June 11, 2022 Mesh to Metahuman in #UE5 @ObscureNerdVR @wandduel pic.twitter.com/VI0KMgUc1X\n\u0026mdash; Pratik Suketu (@pratiksuketu) June 11, 2022 I used this to make Gman into a Metahuman and this is what I got lmao https://t.co/44dJP9q76j pic.twitter.com/yBMKcEGBEU\n\u0026mdash; TurboDumpster 16 (@TrashHeap64) June 10, 2022 ì´ë²ˆì— ë°œí‘œëœ UE5ì˜ Mesh to Metahuman ê¸°ëŠ¥ì„ ì´ìš©í•´ì„œ ê¸°ì¡´ì— ë‚´ê°€ ë§Œë“  ëª¨ë¸ë§ì„ Metahumanìœ¼ë¡œ ë§Œë“¤ì–´ë´¤ëŠ”ë° ì•„ì£¼ ì˜ ì‘ë™í•œë‹¤.\nì¢€ë” í›„ì²˜ë¦¬ íŒŒë¼ë©”í„°ê°€ ìˆìœ¼ë©´ ì¢‹ê² ë‹¤ëŠ” ìƒê°ì´ ë“¤ì§€ë§Œ ì•„ì§ í˜„ì¬ ì§„í–‰í˜•ìœ¼ë¡œ ë°œì „ ì¤‘ì¸ íˆ´ì´ë‹ˆê¹Œ ì „í˜€ ë¬¸ì œë˜ì§€ ì•Šê³ .#UE5 #MetaHuman pic.twitter.com/Sz5lI5WOdv\n\u0026mdash; í•œëŒ€í›ˆ / Daehoon Han (@Hanguny) June 10, 2022 New custom Mesh to Metahuman example in Unreal. Song: \u0026quot;Bezos IV\u0026quot; by Bo Burnham pic.twitter.com/9lJy1E6c66\n\u0026mdash; Brielle Garcia (47%) (@tacolamp) June 10, 2022 The latest UE5 Metahuman Creator update is insane.\nI can\u0026#39;t wait to use this tech in future projects! pic.twitter.com/R3uOpYpMlv\n\u0026mdash; Cinematic Captures (@CineCaptures) June 10, 2022 1-2-3-done. Have fun with the creator. #MetaHuman #UnrealEngine5 #Metaverse pic.twitter.com/h1yxmeJsFD\n\u0026mdash; æˆ‘çš„æ˜Ÿç³»æ˜¯é‡‘ç‰›åº§ (@iamfromtaurus) June 10, 2022 Metahuman new features is insane! pic.twitter.com/7yT9OpDLMd\n\u0026mdash; Qi Wu (@migo1942) June 10, 2022 \u0026lt;3 MetaHuman from old (2003) low poly model. pic.twitter.com/8jOi8FqtQ9\n\u0026mdash; Bad Idea (@BadID_) June 18, 2022 Latest #metahuman version of myself based on Thursdayâ€™s head scan. #Unrealengine5 #Blender3d pic.twitter.com/xEc2T6cs6X\n\u0026mdash; Skinny Lamprey (@skinny_lamprey) June 18, 2022 Terrifying results {3D} Check out this work from @leslievdb! The metahuman creator looks so cool! @UnrealEngine #MetaHumanCreator #metahumans https://t.co/K3FSvnU2CX\n\u0026mdash; BGMcClung (@BGMcClung) June 13, 2022 ãƒ¡ãƒƒã‚·ãƒ¥ã‹ã‚‰ãƒ¡ã‚¿ãƒ’ãƒ¥ãƒ¼ãƒãƒ³ã‚’ä½œã‚Œã‚‹ãƒ„ãƒ¼ãƒ«Mesh to MetaHumanãŒã§ããŸã®ã§ãƒ­ãƒ¼ãƒãƒªã‚¢ãƒ‹ãƒ¡ã‚­ãƒ£ãƒ©ã§è©¦ã—ã¦ã¿ãŸã€‚ã€€ğŸ˜¦ğŸ˜¦é–²è¦§æ³¨æ„ğŸ˜¦ğŸ˜¦#UE5 #Metahuman #ãƒ­ãƒ¼ãƒãƒª #ãƒ›ãƒ©ãƒ¼ pic.twitter.com/mQRKxNdotv\n\u0026mdash; mariaric (@Mariaticmat) June 9, 2022 Ummmm much too fun bringing in a janky mesh and #UE5 doing the rest #MetaHuman pic.twitter.com/96gEHlHPUv\n\u0026mdash; Ryan Corniel (@DukeGunston) June 10, 2022 ã†ã‚ãƒ¼ã„ï¼è‡ªåˆ†ã®ãƒ¢ãƒ‡ãƒ«ã‚’MetaHumanã«ã§ããŸãƒ¼ï¼#MetaHuman pic.twitter.com/DYrudbQSb2\n\u0026mdash; ãã‚ã•ã‚ (@kurosaurus) June 10, 2022 è€³ã‚‚å¾Œé ­éƒ¨ã‚‚ç„¡ã„ã€é¼»ã®é•·ã„å¤©ç‹—ã®ãŠé¢ã§ã§ããŸMetahumanãŒã“ã‚Œ!ğŸ«£#UE5 #MetaHuman pic.twitter.com/3tq0ymO4KN\n\u0026mdash; wajyuu (@wajyuu2) June 10, 2022 Starting a new metahuman with a 3D scan of my front face only was not a good ideaâ€¦ #metahuman #metahuman #UnrealEngine5 pic.twitter.com/TJILgnlnsc\n\u0026mdash; Neoyume (@neoyume) June 10, 2022 It\u0026#39;s me, merio@UnrealEngine\n@quixeltools\n#UE5 #UnrealEngine #unreal #metahumancreator #metahuman #Mario pic.twitter.com/dwDkAxX4x5\n\u0026mdash; EXpMiNi (@EXpMiNi) June 9, 2022 è‡ªåˆ†ã®MetaHumanä½œã‚ŠãŸã‹ã£ãŸã®ã§ã€MetaHuman Pluginã‚’æ—©é€ŸãŠãŸã‚ã—ã€‚#MetaHuman #UE5 pic.twitter.com/ol9UxHRgEw\n\u0026mdash; ovjang (@followapp) June 10, 2022 Everyone is super hype on the new #metahuman scan data to mesh plugin update... but no one told LV that he probably shouldn\u0026#39;t try it... #ue5 #nightmarefuel pic.twitter.com/ifzY1V8jNq\n\u0026mdash; Steve Teeps (@Steveteeps) June 10, 2022 mesh to metahumanæœ€é«˜ã«ã„ã„ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ãƒ„ãƒ¼ãƒ«ã ã‚ˆã­ã€‚å‹•ç‰©ã‚‚ã§ããã†ã ã—ğŸ™„\nãƒˆãƒ©ãƒƒã‚«ãƒ¼ãŒã‚‚ã£ã¨æ¥½ã«æ“ä½œã§ãã‚‹ã¨ä½¿ã„ã‚„ã™ã„ã‘ã©ã€‚\nã‚·ãƒ³ãƒ¡ãƒˆãƒªãƒ¼ã«è‡ªå‹•ã§ã—ã¦ãã‚Œã‚‹ãƒ„ãƒ¼ãƒ«ã¨ã€ãƒˆãƒ©ãƒƒã‚«ãƒ¼ã®GRSãƒãƒ‹ãƒ”ãƒ¥ãƒ¬ãƒ¼ãƒˆæ“ä½œãŒã§ãã‚‹ã‚ˆã†ã«ã—ã¦ã»ã—ã„ãªã€€#UE5 #Metahuman pic.twitter.com/1khKvpmoyD\n\u0026mdash; mariaric (@Mariaticmat) June 18, 2022 well, still a lot of stuff to do manually, this tool is not a magic pill, but it does dirty routine job that took hours and days in a second. Feels like miracle#metahuman #ue5 pic.twitter.com/pL5XGrw3pJ\n\u0026mdash; Zarkua (@artzarkua) June 18, 2022 Yeah, Metahuman creator from @UnrealEngine is working great!! pic.twitter.com/Mz8d6UHAqa\n\u0026mdash; Kino Bar (@KinoBar2) June 12, 2022 Unlocked the ET feature in Metahuman Creator ğŸ˜ #madewithunreal @EpicNinaWard #UE5 #UE5Study pic.twitter.com/MMtjyy1Jmm\n\u0026mdash; Unreal N00b (@HoobTall) June 10, 2022 Ummmm much too fun bringing in a janky mesh and #UE5 doing the rest #MetaHuman pic.twitter.com/96gEHlHPUv\n\u0026mdash; Ryan Corniel (@DukeGunston) June 10, 2022 Tutorials ","date":"2022-06-18","permalink":"https://mcleary.github.io/posts/mesh-to-metahuman-gallery/","tags":["Unreal Engine","Unreal Engine 5","UE5","MetaHuman","Mesh to MetaHuman"],"title":"Mesh to MetaHuman Gallery"},{"content":"The Modern Sponza rendered in Unreal Engine 5 with Lumen The original Sponza scene is a classic in the field of computer graphics research. Originally created by Crytek for use in CRYENGINE for testing rendering features it was quickly adopted by the community making appearances in several academic papers and CG experiments. The original scene can still be downloading from the CRYENGINE Marketplace and, to this day, it is an amazing piece of graphics to look at.\nWith the shift to Physically Based Rendering (PBR) in pretty much all modern games engine, Sponza went a little bit in under the radar but those days are over.\nIntel Graphics Research recently released an updated version of Sponza, complete with PBR materials, extra props and an animated knight in a shinny armour great for testing reflections.\nThe model comes in different formats and its split into Base Scene and Add-on Packages\nDownload the new Sponza Scene From Intel Graphics Research here\nI took the FBX version of the model and loaded it in Unreal Engine 5. Importing it was not the easiest thing in the world as the materials were not created correctly. Using Datasmith is supposed to make the importing experience much better but I decided to stick with FBX to use Unreal Engine 5 as vanilla as possible.\nso after tweaking the materials and setting the light this is my result. Its not perfect by any means but it shows how good realtime is nowadays. Enjoy!\n","date":"2022-04-29","permalink":"https://mcleary.github.io/posts/modern-sponza-ue5/","tags":["Unreal Engine","Unreal Engine 5","UE5","Rendering","Lumen"],"title":"Modern Sponza UE5"},{"content":"\rIn my post A Simple Vulkan Compute Example in C++ I described the hello world of Vulkan Compute. A very simple application that squares a vector of integers using a HLSL compute shader. It is quite an involved process, requiring many steps to be completed before getting to the actual compute shader execution. You need to:\nCreate a Vulkan Instance, Physical Device, Logical Device Find the flags required to create a compute Queue Create the buffers that the shader will operate on: Query the memory requirements for a particular buffer Find the index of the memory type to create the buffer from Allocate Memory for the buffers Map the memory and fill it with the data you want Bind the buffers to the memory Create a Descriptor Set, Shader Module, Pipeline Create a Command Pool, Command Buffer, Fences Dispatch the shader Wait for completion Map the buffers and read the results back It is a rather complex process just to run some program on your GPU.\nI deliberately split the buffer creation steps into multiple sub-steps to highlight how difficult it can be to manage memory in a Vulkan application. Note that the history repeats for DirectX 12. You need to be aware of the different types of memory and you need to be clever when allocating memory and binding buffers to it. Ideally you allocate a large chunk of memory with several buffers bound to it, when buffers get destroyed you just free the memory in the the pool leaving it available for another allocation. Allocations are quite expensive, so avoiding allocating memory is a good idea.\nFortunately there is a super easy-to-use, open-source, MIT licensed library developed by AMD as part of their GPU-Open initiative that helps you to manage memory in your Vulkan application. The Vulkan Memory Allocator, or VMA for short\nThe Vulkan Memory Allocator (VMA) Library Some useful links:\nVulkan Memory Allocator on Github Detailed documentation of the Vulkan Memory Allocator library Basic usage This is a single-header C++ library with a C interface. You can just put the header in your repository and done, installed. You just need to include the following:\n#define VMA_IMPLEMENTATION #include \u0026#34;vk_mem_alloc.h\u0026#34; The VMA_IMPLEMENTATION macro needs to be defined in a single source file or you will get linker errors, so it is recommended that you create a dedicated translation unit just for that.\nYou start by creating an allocator:\nVmaAllocatorCreateInfo AllocatorInfo = {}; AllocatorInfo.vulkanApiVersion = DeviceProps.apiVersion; AllocatorInfo.physicalDevice = PhysicalDevice; AllocatorInfo.device = Device; AllocatorInfo.instance = Instance; VmaAllocator Allocator; vmaCreateAllocator(\u0026amp;AllocatorInfo, \u0026amp;Allocator); Then you can start creating buffers:\nVkBuffer InBufferRaw; VkBuffer OutBufferRaw; VmaAllocationCreateInfo AllocationInfo = {}; AllocationInfo.usage = VMA_MEMORY_USAGE_CPU_TO_GPU; VmaAllocation InBufferAllocation; vmaCreateBuffer(Allocator, \u0026amp;static_cast\u0026lt;VkBufferCreateInfo\u0026gt;(BufferCreateInfo), \u0026amp;AllocationInfo, \u0026amp;InBufferRaw, \u0026amp;InBufferAllocation, nullptr); AllocationInfo.usage = VMA_MEMORY_USAGE_GPU_TO_CPU; VmaAllocation OutBufferAllocation; vmaCreateBuffer(Allocator, \u0026amp;static_cast\u0026lt;VkBufferCreateInfo\u0026gt;(BufferCreateInfo), \u0026amp;AllocationInfo, \u0026amp;OutBufferRaw, \u0026amp;OutBufferAllocation, nullptr); vk::Buffer InBuffer = InBufferRaw; vk::Buffer OutBuffer = OutBufferRaw; int32_t* InBufferPtr = nullptr; vmaMapMemory(Allocator, InBufferAllocation, reinterpret_cast\u0026lt;void**\u0026gt;(\u0026amp;InBufferPtr)); for (int32_t I = 0; I \u0026lt; NumElements; ++I) { InBufferPtr[I] = I; } vmaUnmapMemory(Allocator, InBufferAllocation); Note the reduction of lines of code compared to the raw Vulkan way of allocating memory, the library handles the details for you, and what\u0026rsquo;s best, it does it very efficiently.\nVisualising Memory Allocations One very neat feature of this library is its capability to display your memory allocations in a visual way.\nAt any point in your program you can build a stats string, which is a json file containing the internals of the library. This json can then be fed into a tool that comes with the library to generate an image that shows you how your application is using the memory, you can see more information on the VmaDumpVis on Github, but you can basically do this:\nchar* StatsString = nullptr; vmaBuildStatsString(Allocator, \u0026amp;StatsString, true); { std::ofstream OutStats{ \u0026#34;VmaStats.json\u0026#34; }; OutStats \u0026lt;\u0026lt; StatsString; } vmaFreeStatsString(Allocator, StatsString); And then run the VmaDumpVis.py script with this file to get an image with a snapshot of your memory allocations. Pretty cool!\nTo conclude, this library is super useful, efficient and easy to integrate and you should really use it in your Vulkan application.\nI have updated my Vulkan Compute Sample on Github to include an example of how to integrate VMA in an application. You can disable VMA by simply commenting the line\n// Comment this to disable VMA support #define WITH_VMA This shows how easy it is to replace memory allocations in Vulkan with VMA\n","date":"2021-08-22","permalink":"https://mcleary.github.io/posts/vulkan-memory-allocator/","tags":["vulkan","vulkan-compute","GPGPU","memory"],"title":"Easy Memory Management with the Vulkan Memory Allocator"},{"content":"In Unreal Engine\u0026rsquo;s terminology, a Data Asset is an asset that stores, guess what, data. It can be used, for example, to decouple the configuration from behaviour.\nData Assets in Unreal Engine all inherit from the UDataAsset. You can define a new Data Asset type by inheriting from UDataAsset and then your new data asset will become available in the Pick Data Asset Class Dialog:\nI\u0026rsquo;m used to create new data assets using C++, you inherit from UDataAsset, add some members with UPROPERTY and done, however, recently I wanted to create one using only Blueprints. Looking into existing tutorials on data assets, people usually say that you must create your data asset type in C++ first.\nThat is actually true, but only if you want to inherit directly from UDataAsset. But there is a way to create a new data assets using only blueprints.\nThe procedure is as follows:\nCreate a new Blueprint class that inherits from UPrimaryDataAsset Add variables in your Blueprint Your blueprint class will now be available in the Pick Data Asset Class Dialog. Here is a simple example of a blueprint that inherits from UPrimaryDataAsset with three variables:\nAnd the blueprint type showing up as a new data asset type:\nWhen you create a new asset of this type and double click in it, you can see that all the variables from the blueprint class are now fields of the new data asset:\nSo this is how you create a new data asset type using only blueprints, no C++ involved.\n","date":"2021-07-24","permalink":"https://mcleary.github.io/posts/unreal-data-assets-from-blueprints/","tags":["Unreal Engine","UE4","UE5","Assets","GameDev"],"title":"Creating Unreal Engine Data Assets using only Blueprints"},{"content":"\rVulkan is great. It provides a cross-platform API to write applications that use the GPU to do graphics and general purpose compute. Designed from the ground-up to be a modern API, using Vulkan can be quite difficult so you better know what you\u0026rsquo;re doing if you plan to use Vulkan for your application.\nVulkan provides both a graphics and compute APIs and in this post I will focus on the compute part of it as I\u0026rsquo;m still not very familiar with the graphics side of it.\nI had a difficult time searching for simple Vulkan compute samples as the official Khronos Vulkan-Samples only include more elaborate examples. My goal was to write the minimal amount of code to get a compute shader running using Vulkan.\nI found two very useful resources that do more or less what I wanted:\nA Simple Vulkan Compute Example by Neil Henning Vulkan Compute Example by Slava Savenko Neil\u0026rsquo;s post is great as it goes straight into the code but it uses the C API from Vulkan, so very verbose. I wanted to use vulkan.hpp as it provides a nice C++ interface to use Vulkan. The Vulkan C++ bindings are almost a one-to-one match with the C API, so theoretically I could just port Neil\u0026rsquo;s code to use the Vulkan C++ header but I found this task was not that simple. Slava\u0026rsquo;s sample did just that, however, as with most Vulkan samples out there, there is an infrastructure around the code to make it easier to write, namely classes with a lot of methods to abstract some of the resource management required to use Vulkan.\nDespite that, both links proved to be really useful when writing my own sample.\nThe goal is simple: Run a compute shader that squares the numbers from an input buffer and stores the results in an output buffer, i.e., run the equivalent of the following code but in the GPU, using Vulkan:\nstd::vector\u0026lt;int\u0026gt; Input, Output; for (int I = 0; I \u0026lt; Input.size(); ++I) { Output[I] = Input[I] * Input[I]; } So let\u0026rsquo;s start writing this program.\nInfrastructure Make sure you have the Vulkan SDK installed on your machine.\nI\u0026rsquo;m assuming you have the Vulkan SDK installed on your machine. For this program I\u0026rsquo;m going to use CMake and HLSL for the shader part.\nThe following CMake file can be used to build the program. I chose to compile the shader to SPIR-V ahead of time to make things simple in the C++ side. Using add_custom_target you can create a CMake target that will compile the Square.hlsl shader when building the project.\ncmake_minimum_required(VERSION 3.16) project(VulkanCompute) find_package(Vulkan REQUIRED) add_custom_command( OUTPUT \u0026#34;${CMAKE_BINARY_DIR}/Square.spv\u0026#34; COMMAND $ENV{VK_SDK_PATH}/Bin/dxc -T cs_6_0 -E \u0026#34;Main\u0026#34; -spirv -fvk-use-dx-layout -fspv-target-env=vulkan1.1 -Fo \u0026#34;${CMAKE_BINARY_DIR}/Square.spv\u0026#34; \u0026#34;Square.hlsl\u0026#34; DEPENDS \u0026#34;Square.hlsl\u0026#34; WORKING_DIRECTORY ${CMAKE_CURRENT_SOURCE_DIR} COMMENT \u0026#34;Buiding Shaders\u0026#34; ) add_custom_target(ComputeShader DEPENDS \u0026#34;${CMAKE_BINARY_DIR}/Square.spv\u0026#34;) add_executable(VulkanCompute \u0026#34;main.cpp\u0026#34;) target_link_libraries(VulkanCompute PRIVATE Vulkan::Vulkan) add_dependencies(VulkanCompute ComputeShader) Preamble To use the Vulkan C++ header one just need to\n#include \u0026lt;vulkan/vulkan.hpp\u0026gt; Vulkan Instance - vk::Instance A Vulkan application starts with a vk::Instance, so lets create one:\nvk::ApplicationInfo AppInfo{ \u0026#34;VulkanCompute\u0026#34;, // Application Name 1, // Application Version nullptr, // Engine Name or nullptr 0, // Engine Version VK_API_VERSION_1_1 // Vulkan API version }; const std::vector\u0026lt;const char*\u0026gt; Layers = { \u0026#34;VK_LAYER_KHRONOS_validation\u0026#34; }; vk::InstanceCreateInfo InstanceCreateInfo(vk::InstanceCreateFlags(), // Flags \u0026amp;AppInfo, // Application Info Layers.size(), // Layers count Layers.data()); // Layers vk::Instance Instance = vk::createInstance(InstanceCreateInfo); Here I\u0026rsquo;m enabling the VK_LAYER_KHRONOS_validation layer so we can have some help from Vulkan in case something goes wrong.\nEnumerating the Physical Devices - vk::PhysicalDevice A vk::PhysicalDevice represents, as the name suggests, the physical piece of hardware that we can use to run our application. We need to select a physical device from which we can create a logical device, vk::Device that we use to interact with it:\nvk::PhysicalDevice PhysicalDevice = Instance.enumeratePhysicalDevices().front(); vk::PhysicalDeviceProperties DeviceProps = PhysicalDevice.getProperties(); std::cout \u0026lt;\u0026lt; \u0026#34;Device Name : \u0026#34; \u0026lt;\u0026lt; DeviceProps.deviceName \u0026lt;\u0026lt; std::endl; const uint32_t ApiVersion = DeviceProps.apiVersion; std::cout \u0026lt;\u0026lt; \u0026#34;Vulkan Version : \u0026#34; \u0026lt;\u0026lt; VK_VERSION_MAJOR(ApiVersion) \u0026lt;\u0026lt; \u0026#34;.\u0026#34; \u0026lt;\u0026lt; VK_VERSION_MINOR(ApiVersion) \u0026lt;\u0026lt; \u0026#34;.\u0026#34; \u0026lt;\u0026lt; VK_VERSION_PATCH(ApiVersion); vk::PhysicalDeviceLimits DeviceLimits = DeviceProps.limits; std::cout \u0026lt;\u0026lt; \u0026#34;Max Compute Shared Memory Size: \u0026#34; \u0026lt;\u0026lt; DeviceLimits.maxComputeSharedMemorySize / 1024 \u0026lt;\u0026lt; \u0026#34; KB\u0026#34; \u0026lt;\u0026lt; std::endl; Here I\u0026rsquo;m just printing some information from the first physical device available in the machine.\nQueue Family Index - vk::QueueFamilyProperties We need a vk::Queue where we submit work to be done by the device, hopefully a GPU in this case. Note that extra code is required to make sure the selected device is the one you want, in case there are more than one available.\nVulkan supports different types of queues, so we need to query which queue family we need to create a queue suitable for compute work:\nstd::vector\u0026lt;vk::QueueFamilyProperties\u0026gt; QueueFamilyProps = PhysicalDevice.getQueueFamilyProperties(); auto PropIt = std::find_if(QueueFamilyProps.begin(), QueueFamilyProps.end(), [](const vk::QueueFamilyProperties\u0026amp; Prop) { return Prop.queueFlags \u0026amp; vk::QueueFlagBits::eCompute; }); const uint32_t ComputeQueueFamilyIndex = std::distance(QueueFamilyProps.begin(), PropIt); std::cout \u0026lt;\u0026lt; \u0026#34;Compute Queue Family Index: \u0026#34; \u0026lt;\u0026lt; ComputeQueueFamilyIndex \u0026lt;\u0026lt; std::endl; This will select a queue that has compute capabilities. This is equivalent to searching for a queue with the VK_QUEUE_COMPUTE_BIT flag using the C API.\nVulkan Device - vk::Device Creating a device requires a vk::DeviceQueueCreateInfo and a vk::DeviceCreateInfo:\nvk::DeviceQueueCreateInfo DeviceQueueCreateInfo(vk::DeviceQueueCreateFlags(), // Flags ComputeQueueFamilyIndex, // Queue Family Index 1); // Number of Queues vk::DeviceCreateInfo DeviceCreateInfo(vk::DeviceCreateFlags(), // Flags DeviceQueueCreateInfo); // Device Queue Create Info struct vk::Device Device = PhysicalDevice.createDevice(DeviceCreateInfo); Allocating Memory Allocating memory in Vulkan is a pain. There libraries that facilitate this task like AMD\u0026rsquo;s Vulkan Memory Allocator, but using this libraries is out of the scope for this post, also I wanted to see how to do it manually first.\nVulkan separates buffers memory. Buffers in Vulkan are just a view into a piece of memory that you also need to manually configure. Allocating memory then is split into three parts:\nCreate the required buffers for the application Allocate the memory to back the buffers Bind the buffers to the memory This separation allows programmers to fine tune memory usage by allocating a large chunk of memory for several buffers for example. For the sake of simplicity, each vk::Buffer will have its corresponding vk::Memory associated with it:\nCreating the buffers - vk::Buffer I\u0026rsquo;m going to create two buffers with 10 elements each:\nconst uint32_t NumElements = 10; const uint32_t BufferSize = NumElements * sizeof(int32_t); vk::BufferCreateInfo BufferCreateInfo{ vk::BufferCreateFlags(), // Flags BufferSize, // Size vk::BufferUsageFlagBits::eStorageBuffer, // Usage vk::SharingMode::eExclusive, // Sharing mode 1, // Number of queue family indices \u0026amp;ComputeQueueFamilyIndex // List of queue family indices }; vk::Buffer InBuffer = Device.createBuffer(BufferCreateInfo); vk::Buffer OutBuffer = Device.createBuffer(BufferCreateInfo); Allocating memory To allocate memory in Vulkan we first need to find the type of memory we actually require to back the buffers we have. The vk::Device provides a member function called vk::Device::getBufferMemoryRequirements that returns a vk::MemoryRequirements object with information so we can ask Vulkan how much memory to allocate for each buffer:\nvk::MemoryRequirements InBufferMemoryRequirements = Device.getBufferMemoryRequirements(InBuffer); vk::MemoryRequirements OutBufferMemoryRequirements = Device.getBufferMemoryRequirements(OutBuffer); With this information on hand we can query Vulkan for the memory type required to allocate memory that is visible from the host, i.e., memory that can be mapped on the host side:\nvk::PhysicalDeviceMemoryProperties MemoryProperties = PhysicalDevice.getMemoryProperties(); uint32_t MemoryTypeIndex = uint32_t(~0); vk::DeviceSize MemoryHeapSize = uint32_t(~0); for (uint32_t CurrentMemoryTypeIndex = 0; CurrentMemoryTypeIndex \u0026lt; MemoryProperties.memoryTypeCount; ++CurrentMemoryTypeIndex) { vk::MemoryType MemoryType = MemoryProperties.memoryTypes[CurrentMemoryTypeIndex]; if ((vk::MemoryPropertyFlagBits::eHostVisible \u0026amp; MemoryType.propertyFlags) \u0026amp;\u0026amp; (vk::MemoryPropertyFlagBits::eHostCoherent \u0026amp; MemoryType.propertyFlags)) { MemoryHeapSize = MemoryProperties.memoryHeaps[MemoryType.heapIndex].size; MemoryTypeIndex = CurrentMemoryTypeIndex; break; } } std::cout \u0026lt;\u0026lt; \u0026#34;Memory Type Index: \u0026#34; \u0026lt;\u0026lt; MemoryTypeIndex \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34;Memory Heap Size : \u0026#34; \u0026lt;\u0026lt; MemoryHeapSize / 1024 / 1024 / 1024 \u0026lt;\u0026lt; \u0026#34; GB\u0026#34; \u0026lt;\u0026lt; std::endl; And finally we can ask the device to allocate the required memory for our buffers:\nvk::MemoryAllocateInfo InBufferMemoryAllocateInfo(InBufferMemoryRequirements.size, MemoryTypeIndex); vk::MemoryAllocateInfo OutBufferMemoryAllocateInfo(OutBufferMemoryRequirements.size, MemoryTypeIndex); vk::DeviceMemory InBufferMemory = Device.allocateMemory(InBufferMemoryAllocateInfo); vk::DeviceMemory OutBufferMemory = Device.allocateMemory(InBufferMemoryAllocateInfo); The last step of the memory allocation part is to get a mapped pointer to this memory that can be used to copy data from the host to the device. For this simple example I\u0026rsquo;m just setting the value of each element as its index:\nint32_t* InBufferPtr = static_cast\u0026lt;int32_t*\u0026gt;(Device.mapMemory(InBufferMemory, 0, BufferSize)); for (int32_t I = 0; I \u0026lt; NumElements; ++I) { InBufferPtr[I] = I; } Device.unmapMemory(InBufferMemory); Binding Buffers to Memory Finally we can bind the buffers to the allocated memory:\nDevice.bindBufferMemory(InBuffer, InBufferMemory, 0); Device.bindBufferMemory(OutBuffer, OutBufferMemory, 0); And that concludes the memory allocation part of the program.\nCreating the Compute Pipeline - vk::Pipeline The next part of the process is to create the compute pipeline that will be used to run the compute shader on the GPU. Lets start with the HLSL shader we are going to run:\n[[vk::binding(0, 0)]] RWStructuredBuffer\u0026lt;int\u0026gt; InBuffer;\r[[vk::binding(1, 0)]] RWStructuredBuffer\u0026lt;int\u0026gt; OutBuffer;\r[numthreads(1, 1, 1)]\rvoid Main(uint3 DTid : SV_DispatchThreadID)\r{\rOutBuffer[DTid.x] = InBuffer[DTid.x] * InBuffer[DTid.x];\r} The only difference from a regular HLSL shader are the annotations for each buffer. [[vk::binding(1, 0)]] means that a buffer will use binding 1 from descriptor set 0. Descriptor sets can be thought of a way to tell Vulkan how the buffers we defined in the previous section are going to be passed to the pipeline. If you have an existing HLSL shader that you want to use but you can\u0026rsquo;t change the its source code to have the bindings, you can still set the bindings in the command line using dxc using the -fvk-bind-globals argument.\nWith the CMake setup defined in the beginning of the post, there will be a file called Square.spv in the build folder for the project. This file contains the SPIR-V bytecode representing this shader. The goal here is to load this shader as the compute stage of our pipeline.\nShader Module - vk::ShaderModule We start by reading the contents of the SPIR-V file and creating a vk::ShaderModule:\nstd::vector\u0026lt;char\u0026gt; ShaderContents; if (std::ifstream ShaderFile{ \u0026#34;Square.spv\u0026#34;, std::ios::binary | std::ios::ate }) { const size_t FileSize = ShaderFile.tellg(); ShaderFile.seekg(0); ShaderContents.resize(FileSize, \u0026#39;\\0\u0026#39;); ShaderFile.read(ShaderContents.data(), FileSize); } vk::ShaderModuleCreateInfo ShaderModuleCreateInfo( vk::ShaderModuleCreateFlags(), // Flags ShaderContents.size(), // Code size reinterpret_cast\u0026lt;const uint32_t*\u0026gt;(ShaderContents.data())); // Code vk::ShaderModule ShaderModule = Device.createShaderModule(ShaderModuleCreateInfo); Descriptor Set Layout - vk::DescriptorSetLayout Next we define the vk::DescriptorSetLayout. This object will tell Vulkan the layout of data to be passed into to the pipeline. Note that this is not the actual descriptor set, it is just the layout of the thing. This got me confused when I first saw it. The actual descriptor set is represented by a vk::DescriptorSet and it needs to be allocated using a descriptor pool. Vulkan is complicated, but you have control!\nLet\u0026rsquo;s define the vk::DescriptorSetLayout:\nconst std::vector\u0026lt;vk::DescriptorSetLayoutBinding\u0026gt; DescriptorSetLayoutBinding = { {0, vk::DescriptorType::eStorageBuffer, 1, vk::ShaderStageFlagBits::eCompute}, {1, vk::DescriptorType::eStorageBuffer, 1, vk::ShaderStageFlagBits::eCompute} }; vk::DescriptorSetLayoutCreateInfo DescriptorSetLayoutCreateInfo( vk::DescriptorSetLayoutCreateFlags(), DescriptorSetLayoutBinding); vk::DescriptorSetLayout DescriptorSetLayout = Device.createDescriptorSetLayout(DescriptorSetLayoutCreateInfo); The vk::DescriptorSetLayout is specified using a series of vk::DescriptorSetLayoutBinding objects. Each binding will assign an index to a buffer in the pipeline. With the vk::DescriptorSetLayout created we can move to create the layout of our compute pipeline. Yes, another layout, not the actual thing!\nPipeline Layout - vk::PipelineLayout vk::PipelineLayoutCreateInfo PipelineLayoutCreateInfo(vk::PipelineLayoutCreateFlags(), DescriptorSetLayout); vk::PipelineLayout PipelineLayout = Device.createPipelineLayout(PipelineLayoutCreateInfo); vk::PipelineCache PipelineCache = Device.createPipelineCache(vk::PipelineCacheCreateInfo()); Pipeline - vk::Pipeline Now we can finally create the compute pipeline:\nvk::PipelineShaderStageCreateInfo PipelineShaderCreateInfo( vk::PipelineShaderStageCreateFlags(), // Flags vk::ShaderStageFlagBits::eCompute, // Stage ShaderModule, // Shader Module \u0026#34;Main\u0026#34;); // Shader Entry Point vk::ComputePipelineCreateInfo ComputePipelineCreateInfo( vk::PipelineCreateFlags(), // Flags PipelineShaderCreateInfo, // Shader Create Info struct PipelineLayout); // Pipeline Layout vk::Pipeline ComputePipeline = Device.createComputePipeline(PipelineCache, ComputePipelineCreateInfo); Creating the vk::DescriptorSet Descriptor sets must be allocated in a vk::DescriptorPool, so we need to create one first:\nvk::DescriptorPoolSize DescriptorPoolSize(vk::DescriptorType::eStorageBuffer, 2); vk::DescriptorPoolCreateInfo DescriptorPoolCreateInfo(vk::DescriptorPoolCreateFlags(), 1, DescriptorPoolSize); vk::DescriptorPool DescriptorPool = Device.createDescriptorPool(DescriptorPoolCreateInfo); Now we can finally allocate the descriptor set and update them to use our buffers:\nvk::DescriptorSetAllocateInfo DescriptorSetAllocInfo(DescriptorPool, 1, \u0026amp;DescriptorSetLayout); const std::vector\u0026lt;vk::DescriptorSet\u0026gt; DescriptorSets = Device.allocateDescriptorSets(DescriptorSetAllocInfo); vk::DescriptorSet DescriptorSet = DescriptorSets.front(); vk::DescriptorBufferInfo InBufferInfo(InBuffer, 0, NumElements * sizeof(int32_t)); vk::DescriptorBufferInfo OutBufferInfo(OutBuffer, 0, NumElements * sizeof(int32_t)); const std::vector\u0026lt;vk::WriteDescriptorSet\u0026gt; WriteDescriptorSets = { {DescriptorSet, 0, 0, 1, vk::DescriptorType::eStorageBuffer, nullptr, \u0026amp;InBufferInfo}, {DescriptorSet, 1, 0, 1, vk::DescriptorType::eStorageBuffer, nullptr, \u0026amp;OutBufferInfo}, }; Device.updateDescriptorSets(WriteDescriptorSets, {}); Submitting the work to the GPU Command Pool - vk::CommandPool To actually run this shader on the GPU we need to submit the work on a vk::Queue. We tell the queue to commands stored in a one or more vk::CommandBuffers. Commands must be allocated in a vk::CommandPool, so we need to create one first:\nvk::CommandPoolCreateInfo CommandPoolCreateInfo(vk::CommandPoolCreateFlags(), ComputeQueueFamilyIndex); vk::CommandPool CommandPool = Device.createCommandPool(CommandPoolCreateInfo); Command Buffers - vk::CommandBuffer Now we can use the command pool to allocate one or more command buffers:\nvk::CommandBufferAllocateInfo CommandBufferAllocInfo( CommandPool, // Command Pool vk::CommandBufferLevel::ePrimary, // Level 1); // Num Command Buffers const std::vector\u0026lt;vk::CommandBuffer\u0026gt; CmdBuffers = Device.allocateCommandBuffers(CommandBufferAllocInfo); vk::CommandBuffer CmdBuffer = CmdBuffers.front(); Recording Commands We can now record commands in the vk::CommandBuffer object. To run the compute shader we need to bind the pipeline, descriptor sets and record a vk::CommandBuffer::dispatch call:\nvk::CommandBufferBeginInfo CmdBufferBeginInfo(vk::CommandBufferUsageFlagBits::eOneTimeSubmit); CmdBuffer.begin(CmdBufferBeginInfo); CmdBuffer.bindPipeline(vk::PipelineBindPoint::eCompute, ComputePipeline); CmdBuffer.bindDescriptorSets(vk::PipelineBindPoint::eCompute, // Bind point PipelineLayout, // Pipeline Layout 0, // First descriptor set { DescriptorSet }, // List of descriptor sets {}); // Dynamic offsets CmdBuffer.dispatch(NumElements, 1, 1); CmdBuffer.end(); The vk::CommandBuffer::dispatch function takes the number of threads to launch in the device. In this example we are launching one thread for element.\nWith the vk::CommmandBuffer recorded we can finaly submit the work the GPU. We first get the vk::Queue from the vk::Device using the queue family index retrieved earlier and we create a vk::Fence. The fence is a mechanism we can use to wait for the compute shader to complete. After waiting we can read the results of our computation:\nvk::Queue Queue = Device.getQueue(ComputeQueueFamilyIndex, 0); vk::Fence Fence = Device.createFence(vk::FenceCreateInfo()); vk::SubmitInfo SubmitInfo(0, // Num Wait Semaphores nullptr, // Wait Semaphores nullptr, // Pipeline Stage Flags 1, // Num Command Buffers \u0026amp;CmdBuffer); // List of command buffers Queue.submit({ SubmitInfo }, Fence); Device.waitForFences({ Fence }, // List of fences true, // Wait All uint64_t(-1)); // Timeout The final step is to map the output buffer and read the results. For this example we are just going to print the values in the terminal:\nInBufferPtr = static_cast\u0026lt;int32_t*\u0026gt;(Device.mapMemory(InBufferMemory, 0, BufferSize)); for (uint32_t I = 0; I \u0026lt; NumElements; ++I) { std::cout \u0026lt;\u0026lt; InBufferPtr[I] \u0026lt;\u0026lt; \u0026#34; \u0026#34;; } std::cout \u0026lt;\u0026lt; std::endl; Device.unmapMemory(InBufferMemory); int32_t* OutBufferPtr = static_cast\u0026lt;int32_t*\u0026gt;(Device.mapMemory(OutBufferMemory, 0, BufferSize)); for (uint32_t I = 0; I \u0026lt; NumElements; ++I) { std::cout \u0026lt;\u0026lt; OutBufferPtr[I] \u0026lt;\u0026lt; \u0026#34; \u0026#34;; } std::cout \u0026lt;\u0026lt; std::endl; Device.unmapMemory(OutBufferMemory); Cleaning We need to manually delete the resources used by our program or the validation layer will shout at you:\nDevice.resetCommandPool(CommandPool, vk::CommandPoolResetFlags()); Device.destroyFence(Fence); Device.destroyDescriptorSetLayout(DescriptorSetLayout); Device.destroyPipelineLayout(PipelineLayout); Device.destroyPipelineCache(PipelineCache); Device.destroyShaderModule(ShaderModule); Device.destroyPipeline(ComputePipeline); Device.destroyDescriptorPool(DescriptorPool); Device.destroyCommandPool(CommandPool); Device.freeMemory(InBufferMemory); Device.freeMemory(OutBufferMemory); Device.destroyBuffer(InBuffer); Device.destroyBuffer(OutBuffer); Device.destroy(); Instance.destroy(); The Vulkan C++ Header also has a raii set of objects that can be used to avoid manually cleaning resources, for example:\nvk::raii::Context context; vk::raii::Instance instance = vk::raii::su::makeInstance( context, AppName, EngineName ); // enumerate the physicalDevices vk::raii::PhysicalDevices physicalDevices( instance ); So this objects will clean their resources upon destruction.\nConclusion If you are here, well done, you can now use Vulkan to square a few numbers on the GPU =). Hopefully this will be useful to you in staring with Vulkan to run compute workloads on your GPU.\nThis code is pretty much in the order required to build your application so you should be able to copy-and-paste this code into your main function and have some numbers printed in the terminal. You can also find this code in this Github Repo ready to be built and executed.\nSee you next time!\n","date":"2021-07-07","permalink":"https://mcleary.github.io/posts/vulkan-compute-example/","tags":["vulkan","vulkan-compute","GPGPU"],"title":"A Simple Vulkan Compute Example in C++"}]